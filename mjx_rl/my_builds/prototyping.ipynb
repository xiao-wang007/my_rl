{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2118bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these installs in your terminal if not already installed:\n",
    "# pip install mujoco mujoco_mjx brax\n",
    "\n",
    "# Verify packages are available\n",
    "import mujoco\n",
    "import mujoco.mjx\n",
    "print(\"MuJoCo and MJX installed successfully\")\n",
    "\n",
    "# Check if MuJoCo installation was successful\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Check for GPU availability\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print('GPU detected via nvidia-smi')\n",
    "        # Set MuJoCo to use EGL rendering backend (for GPU)\n",
    "        os.environ['MUJOCO_GL'] = 'egl'\n",
    "    else:\n",
    "        print('No NVIDIA GPU detected, using CPU rendering')\n",
    "        os.environ['MUJOCO_GL'] = 'osmesa'  # Software rendering fallback\n",
    "except FileNotFoundError:\n",
    "    print('nvidia-smi not found, using CPU rendering')\n",
    "    os.environ['MUJOCO_GL'] = 'osmesa'  # Software rendering fallback\n",
    "\n",
    "# Verify MuJoCo works\n",
    "try:\n",
    "    print('Checking that the installation succeeded:')\n",
    "    import mujoco\n",
    "    mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "    print('MuJoCo installation successful.')\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        'Something went wrong during MuJoCo installation. '\n",
    "        'Make sure MuJoCo is properly installed: pip install mujoco mujoco_mjx'\n",
    "    ) from e\n",
    "\n",
    "# Optional: Enable Triton GEMM for better GPU performance (if GPU available)\n",
    "if os.environ.get('MUJOCO_GL') == 'egl':\n",
    "    xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "    xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "    os.environ['XLA_FLAGS'] = xla_flags\n",
    "    print('Enabled Triton GEMM for GPU acceleration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68aab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for plotting and creating graphics\n",
    "# Note: Install ffmpeg via homebrew if not available: brew install ffmpeg\n",
    "# Note: Install mediapy via pip if not available: pip install mediapy\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from typing import Callable, NamedTuple, Optional, Union, List\n",
    "\n",
    "# Graphics and plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import mediapy as media\n",
    "except ImportError:\n",
    "    raise ImportError(\"mediapy not found. Install via: pip install mediapy\")\n",
    "\n",
    "# More legible printing from numpy.\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88754ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import MuJoCo, MJX, and Brax\n",
    "from datetime import datetime\n",
    "from etils import epath\n",
    "import functools\n",
    "from IPython.display import HTML\n",
    "from typing import Any, Dict, Sequence, Tuple, Union\n",
    "import os\n",
    "from ml_collections import config_dict\n",
    "\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "from flax.training import orbax_utils\n",
    "from flax import struct\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapy as media\n",
    "from orbax import checkpoint as ocp\n",
    "\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.base import State as PipelineState\n",
    "from brax.envs.base import Env, PipelineEnv, State\n",
    "from brax.mjx.base import State as MjxState\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, mjcf, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61a862",
   "metadata": {},
   "source": [
    "# Adapt this for domain randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab601c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_randomize(sys, rng):\n",
    "  \"\"\"Randomizes the mjx.Model.\"\"\"\n",
    "  @jax.vmap\n",
    "  def rand(rng):\n",
    "    _, key = jax.random.split(rng, 2)\n",
    "    # friction\n",
    "    friction = jax.random.uniform(key, (1,), minval=0.6, maxval=1.4)\n",
    "    friction = sys.geom_friction.at[:, 0].set(friction)\n",
    "    # actuator\n",
    "    _, key = jax.random.split(key, 2)\n",
    "    gain_range = (-5, 5)\n",
    "    param = jax.random.uniform(\n",
    "        key, (1,), minval=gain_range[0], maxval=gain_range[1]\n",
    "    ) + sys.actuator_gainprm[:, 0]\n",
    "    gain = sys.actuator_gainprm.at[:, 0].set(param)\n",
    "    bias = sys.actuator_biasprm.at[:, 1].set(-param)\n",
    "    return friction, gain, bias\n",
    "\n",
    "  friction, gain, bias = rand(rng)\n",
    "\n",
    "  in_axes = jax.tree_util.tree_map(lambda x: None, sys)\n",
    "  in_axes = in_axes.tree_replace({\n",
    "      'geom_friction': 0,\n",
    "      'actuator_gainprm': 0,\n",
    "      'actuator_biasprm': 0,\n",
    "  })\n",
    "\n",
    "  sys = sys.tree_replace({\n",
    "      'geom_friction': friction,\n",
    "      'actuator_gainprm': gain,\n",
    "      'actuator_biasprm': bias,\n",
    "  })\n",
    "\n",
    "  return sys, in_axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4eec5a",
   "metadata": {},
   "source": [
    "# Adapt this for creating custom env\n",
    "### don't use wrapper around the BraxToGymnaxWrapper! Write my own (but learn from this example) to fit to PureJaxRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56398f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/google-deepmind/mujoco_menagerie\n",
    "\n",
    "#@title Barkour vb Quadruped Env\n",
    "\n",
    "BARKOUR_ROOT_PATH = epath.Path('mujoco_menagerie/google_barkour_vb')\n",
    "\n",
    "\n",
    "def get_config():\n",
    "  \"\"\"Returns reward config for barkour quadruped environment.\"\"\"\n",
    "\n",
    "  def get_default_rewards_config():\n",
    "    default_config = config_dict.ConfigDict(\n",
    "        dict(\n",
    "            # The coefficients for all reward terms used for training. All\n",
    "            # physical quantities are in SI units, if no otherwise specified,\n",
    "            # i.e. joint positions are in rad, positions are measured in meters,\n",
    "            # torques in Nm, and time in seconds, and forces in Newtons.\n",
    "            scales=config_dict.ConfigDict(\n",
    "                dict(\n",
    "                    # Tracking rewards are computed using exp(-delta^2/sigma)\n",
    "                    # sigma can be a hyperparameters to tune.\n",
    "                    # Track the base x-y velocity (no z-velocity tracking.)\n",
    "                    tracking_lin_vel=1.5,\n",
    "                    # Track the angular velocity along z-axis, i.e. yaw rate.\n",
    "                    tracking_ang_vel=0.8,\n",
    "                    # Below are regularization terms, we roughly divide the\n",
    "                    # terms to base state regularizations, joint\n",
    "                    # regularizations, and other behavior regularizations.\n",
    "                    # Penalize the base velocity in z direction, L2 penalty.\n",
    "                    lin_vel_z=-2.0,\n",
    "                    # Penalize the base roll and pitch rate. L2 penalty.\n",
    "                    ang_vel_xy=-0.05,\n",
    "                    # Penalize non-zero roll and pitch angles. L2 penalty.\n",
    "                    orientation=-5.0,\n",
    "                    # L2 regularization of joint torques, |tau|^2.\n",
    "                    torques=-0.0002,\n",
    "                    # Penalize the change in the action and encourage smooth\n",
    "                    # actions. L2 regularization |action - last_action|^2\n",
    "                    action_rate=-0.01,\n",
    "                    # Encourage long swing steps.  However, it does not\n",
    "                    # encourage high clearances.\n",
    "                    feet_air_time=0.2,\n",
    "                    # Encourage no motion at zero command, L2 regularization\n",
    "                    # |q - q_default|^2.\n",
    "                    stand_still=-0.5,\n",
    "                    # Early termination penalty.\n",
    "                    termination=-1.0,\n",
    "                    # Penalizing foot slipping on the ground.\n",
    "                    foot_slip=-0.1,\n",
    "                )\n",
    "            ),\n",
    "            # Tracking reward = exp(-error^2/sigma).\n",
    "            tracking_sigma=0.25,\n",
    "        )\n",
    "    )\n",
    "    return default_config\n",
    "\n",
    "  default_config = config_dict.ConfigDict(\n",
    "      dict(\n",
    "          rewards=get_default_rewards_config(),\n",
    "      )\n",
    "  )\n",
    "\n",
    "  return default_config\n",
    "\n",
    "\n",
    "class BarkourEnv(PipelineEnv):\n",
    "  \"\"\"Environment for training the barkour quadruped joystick policy in MJX.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      obs_noise: float = 0.05,\n",
    "      action_scale: float = 0.3,\n",
    "      kick_vel: float = 0.05,\n",
    "      scene_file: str = 'scene_mjx.xml',\n",
    "      **kwargs,\n",
    "  ):\n",
    "    path = BARKOUR_ROOT_PATH / scene_file\n",
    "    sys = mjcf.load(path.as_posix())\n",
    "    self._dt = 0.02  # this environment is 50 fps\n",
    "    sys = sys.tree_replace({'opt.timestep': 0.004})\n",
    "\n",
    "    # override menagerie params for smoother policy\n",
    "    sys = sys.replace(\n",
    "        dof_damping=sys.dof_damping.at[6:].set(0.5239),\n",
    "        actuator_gainprm=sys.actuator_gainprm.at[:, 0].set(35.0),\n",
    "        actuator_biasprm=sys.actuator_biasprm.at[:, 1].set(-35.0),\n",
    "    )\n",
    "\n",
    "    n_frames = kwargs.pop('n_frames', int(self._dt / sys.opt.timestep))\n",
    "    super().__init__(sys, backend='mjx', n_frames=n_frames)\n",
    "\n",
    "    self.reward_config = get_config()\n",
    "    # set custom from kwargs\n",
    "    for k, v in kwargs.items():\n",
    "      if k.endswith('_scale'):\n",
    "        self.reward_config.rewards.scales[k[:-6]] = v\n",
    "\n",
    "    self._torso_idx = mujoco.mj_name2id(\n",
    "        sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, 'torso'\n",
    "    )\n",
    "    self._action_scale = action_scale\n",
    "    self._obs_noise = obs_noise\n",
    "    self._kick_vel = kick_vel\n",
    "    self._init_q = jp.array(sys.mj_model.keyframe('home').qpos)\n",
    "    self._default_pose = sys.mj_model.keyframe('home').qpos[7:]\n",
    "    self.lowers = jp.array([-0.7, -1.0, 0.05] * 4)\n",
    "    self.uppers = jp.array([0.52, 2.1, 2.1] * 4)\n",
    "    feet_site = [\n",
    "        'foot_front_left',\n",
    "        'foot_hind_left',\n",
    "        'foot_front_right',\n",
    "        'foot_hind_right',\n",
    "    ]\n",
    "    feet_site_id = [\n",
    "        mujoco.mj_name2id(sys.mj_model, mujoco.mjtObj.mjOBJ_SITE.value, f)\n",
    "        for f in feet_site\n",
    "    ]\n",
    "    assert not any(id_ == -1 for id_ in feet_site_id), 'Site not found.'\n",
    "    self._feet_site_id = np.array(feet_site_id)\n",
    "    lower_leg_body = [\n",
    "        'lower_leg_front_left',\n",
    "        'lower_leg_hind_left',\n",
    "        'lower_leg_front_right',\n",
    "        'lower_leg_hind_right',\n",
    "    ]\n",
    "    lower_leg_body_id = [\n",
    "        mujoco.mj_name2id(sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, l)\n",
    "        for l in lower_leg_body\n",
    "    ]\n",
    "    assert not any(id_ == -1 for id_ in lower_leg_body_id), 'Body not found.'\n",
    "    self._lower_leg_body_id = np.array(lower_leg_body_id)\n",
    "    self._foot_radius = 0.0175\n",
    "    self._nv = sys.nv\n",
    "\n",
    "  def sample_command(self, rng: jax.Array) -> jax.Array:\n",
    "    lin_vel_x = [-0.6, 1.5]  # min max [m/s]\n",
    "    lin_vel_y = [-0.8, 0.8]  # min max [m/s]\n",
    "    ang_vel_yaw = [-0.7, 0.7]  # min max [rad/s]\n",
    "\n",
    "    _, key1, key2, key3 = jax.random.split(rng, 4)\n",
    "    lin_vel_x = jax.random.uniform(\n",
    "        key1, (1,), minval=lin_vel_x[0], maxval=lin_vel_x[1]\n",
    "    )\n",
    "    lin_vel_y = jax.random.uniform(\n",
    "        key2, (1,), minval=lin_vel_y[0], maxval=lin_vel_y[1]\n",
    "    )\n",
    "    ang_vel_yaw = jax.random.uniform(\n",
    "        key3, (1,), minval=ang_vel_yaw[0], maxval=ang_vel_yaw[1]\n",
    "    )\n",
    "    new_cmd = jp.array([lin_vel_x[0], lin_vel_y[0], ang_vel_yaw[0]])\n",
    "    return new_cmd\n",
    "\n",
    "  def reset(self, rng: jax.Array) -> State:  # pytype: disable=signature-mismatch\n",
    "    rng, key = jax.random.split(rng)\n",
    "\n",
    "    pipeline_state = self.pipeline_init(self._init_q, jp.zeros(self._nv))\n",
    "\n",
    "    state_info = {\n",
    "        'rng': rng,\n",
    "        'last_act': jp.zeros(12),\n",
    "        'last_vel': jp.zeros(12),\n",
    "        'command': self.sample_command(key),\n",
    "        'last_contact': jp.zeros(4, dtype=bool),\n",
    "        'feet_air_time': jp.zeros(4),\n",
    "        'rewards': {k: 0.0 for k in self.reward_config.rewards.scales.keys()},\n",
    "        'kick': jp.array([0.0, 0.0]),\n",
    "        'step': 0,\n",
    "    }\n",
    "\n",
    "    obs_history = jp.zeros(15 * 31)  # store 15 steps of history\n",
    "    obs = self._get_obs(pipeline_state, state_info, obs_history)\n",
    "    reward, done = jp.zeros(2)\n",
    "    metrics = {'total_dist': 0.0}\n",
    "    for k in state_info['rewards']:\n",
    "      metrics[k] = state_info['rewards'][k]\n",
    "    state = State(pipeline_state, obs, reward, done, metrics, state_info)  # pytype: disable=wrong-arg-types\n",
    "    return state\n",
    "\n",
    "  def step(self, state: State, action: jax.Array) -> State:  # pytype: disable=signature-mismatch\n",
    "    rng, cmd_rng, kick_noise_2 = jax.random.split(state.info['rng'], 3)\n",
    "\n",
    "    # kick\n",
    "    push_interval = 10\n",
    "    kick_theta = jax.random.uniform(kick_noise_2, maxval=2 * jp.pi)\n",
    "    kick = jp.array([jp.cos(kick_theta), jp.sin(kick_theta)])\n",
    "    kick *= jp.mod(state.info['step'], push_interval) == 0\n",
    "    qvel = state.pipeline_state.qvel  # pytype: disable=attribute-error\n",
    "    qvel = qvel.at[:2].set(kick * self._kick_vel + qvel[:2])\n",
    "    state = state.tree_replace({'pipeline_state.qvel': qvel})\n",
    "\n",
    "    # physics step\n",
    "    motor_targets = self._default_pose + action * self._action_scale\n",
    "    motor_targets = jp.clip(motor_targets, self.lowers, self.uppers)\n",
    "    pipeline_state = self.pipeline_step(state.pipeline_state, motor_targets)\n",
    "    x, xd = pipeline_state.x, pipeline_state.xd\n",
    "\n",
    "    # observation data\n",
    "    obs = self._get_obs(pipeline_state, state.info, state.obs)\n",
    "    joint_angles = pipeline_state.q[7:]\n",
    "    joint_vel = pipeline_state.qd[6:]\n",
    "\n",
    "    # foot contact data based on z-position\n",
    "    foot_pos = pipeline_state.site_xpos[self._feet_site_id]  # pytype: disable=attribute-error\n",
    "    foot_contact_z = foot_pos[:, 2] - self._foot_radius\n",
    "    contact = foot_contact_z < 1e-3  # a mm or less off the floor\n",
    "    contact_filt_mm = contact | state.info['last_contact']\n",
    "    contact_filt_cm = (foot_contact_z < 3e-2) | state.info['last_contact']\n",
    "    first_contact = (state.info['feet_air_time'] > 0) * contact_filt_mm\n",
    "    state.info['feet_air_time'] += self.dt\n",
    "\n",
    "    # done if joint limits are reached or robot is falling\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    done = jp.dot(math.rotate(up, x.rot[self._torso_idx - 1]), up) < 0\n",
    "    done |= jp.any(joint_angles < self.lowers)\n",
    "    done |= jp.any(joint_angles > self.uppers)\n",
    "    done |= pipeline_state.x.pos[self._torso_idx - 1, 2] < 0.18\n",
    "\n",
    "    # reward\n",
    "    rewards = {\n",
    "        'tracking_lin_vel': (\n",
    "            self._reward_tracking_lin_vel(state.info['command'], x, xd)\n",
    "        ),\n",
    "        'tracking_ang_vel': (\n",
    "            self._reward_tracking_ang_vel(state.info['command'], x, xd)\n",
    "        ),\n",
    "        'lin_vel_z': self._reward_lin_vel_z(xd),\n",
    "        'ang_vel_xy': self._reward_ang_vel_xy(xd),\n",
    "        'orientation': self._reward_orientation(x),\n",
    "        'torques': self._reward_torques(pipeline_state.qfrc_actuator),  # pytype: disable=attribute-error\n",
    "        'action_rate': self._reward_action_rate(action, state.info['last_act']),\n",
    "        'stand_still': self._reward_stand_still(\n",
    "            state.info['command'], joint_angles,\n",
    "        ),\n",
    "        'feet_air_time': self._reward_feet_air_time(\n",
    "            state.info['feet_air_time'],\n",
    "            first_contact,\n",
    "            state.info['command'],\n",
    "        ),\n",
    "        'foot_slip': self._reward_foot_slip(pipeline_state, contact_filt_cm),\n",
    "        'termination': self._reward_termination(done, state.info['step']),\n",
    "    }\n",
    "    rewards = {\n",
    "        k: v * self.reward_config.rewards.scales[k] for k, v in rewards.items()\n",
    "    }\n",
    "    reward = jp.clip(sum(rewards.values()) * self.dt, 0.0, 10000.0)\n",
    "\n",
    "    # state management\n",
    "    state.info['kick'] = kick\n",
    "    state.info['last_act'] = action\n",
    "    state.info['last_vel'] = joint_vel\n",
    "    state.info['feet_air_time'] *= ~contact_filt_mm\n",
    "    state.info['last_contact'] = contact\n",
    "    state.info['rewards'] = rewards\n",
    "    state.info['step'] += 1\n",
    "    state.info['rng'] = rng\n",
    "\n",
    "    # sample new command if more than 500 timesteps achieved\n",
    "    state.info['command'] = jp.where(\n",
    "        state.info['step'] > 500,\n",
    "        self.sample_command(cmd_rng),\n",
    "        state.info['command'],\n",
    "    )\n",
    "    # reset the step counter when done\n",
    "    state.info['step'] = jp.where(\n",
    "        done | (state.info['step'] > 500), 0, state.info['step']\n",
    "    )\n",
    "\n",
    "    # log total displacement as a proxy metric\n",
    "    state.metrics['total_dist'] = math.normalize(x.pos[self._torso_idx - 1])[1]\n",
    "    state.metrics.update(state.info['rewards'])\n",
    "\n",
    "    done = jp.float32(done)\n",
    "    state = state.replace(\n",
    "        pipeline_state=pipeline_state, obs=obs, reward=reward, done=done\n",
    "    )\n",
    "    return state\n",
    "\n",
    "  def _get_obs(\n",
    "      self,\n",
    "      pipeline_state: base.State,\n",
    "      state_info: dict[str, Any],\n",
    "      obs_history: jax.Array,\n",
    "  ) -> jax.Array:\n",
    "    inv_torso_rot = math.quat_inv(pipeline_state.x.rot[0])\n",
    "    local_rpyrate = math.rotate(pipeline_state.xd.ang[0], inv_torso_rot)\n",
    "\n",
    "    obs = jp.concatenate([\n",
    "        jp.array([local_rpyrate[2]]) * 0.25,                 # yaw rate\n",
    "        math.rotate(jp.array([0, 0, -1]), inv_torso_rot),    # projected gravity\n",
    "        state_info['command'] * jp.array([2.0, 2.0, 0.25]),  # command\n",
    "        pipeline_state.q[7:] - self._default_pose,           # motor angles\n",
    "        state_info['last_act'],                              # last action\n",
    "    ])\n",
    "\n",
    "    # clip, noise\n",
    "    obs = jp.clip(obs, -100.0, 100.0) + self._obs_noise * jax.random.uniform(\n",
    "        state_info['rng'], obs.shape, minval=-1, maxval=1\n",
    "    )\n",
    "    # stack observations through time\n",
    "    obs = jp.roll(obs_history, obs.size).at[:obs.size].set(obs)\n",
    "\n",
    "    return obs\n",
    "\n",
    "  # ------------ reward functions----------------\n",
    "  def _reward_lin_vel_z(self, xd: Motion) -> jax.Array:\n",
    "    # Penalize z axis base linear velocity\n",
    "    return jp.square(xd.vel[0, 2])\n",
    "\n",
    "  def _reward_ang_vel_xy(self, xd: Motion) -> jax.Array:\n",
    "    # Penalize xy axes base angular velocity\n",
    "    return jp.sum(jp.square(xd.ang[0, :2]))\n",
    "\n",
    "  def _reward_orientation(self, x: Transform) -> jax.Array:\n",
    "    # Penalize non flat base orientation\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    rot_up = math.rotate(up, x.rot[0])\n",
    "    return jp.sum(jp.square(rot_up[:2]))\n",
    "\n",
    "  def _reward_torques(self, torques: jax.Array) -> jax.Array:\n",
    "    # Penalize torques\n",
    "    return jp.sqrt(jp.sum(jp.square(torques))) + jp.sum(jp.abs(torques))\n",
    "\n",
    "  def _reward_action_rate(\n",
    "      self, act: jax.Array, last_act: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # Penalize changes in actions\n",
    "    return jp.sum(jp.square(act - last_act))\n",
    "\n",
    "  def _reward_tracking_lin_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion\n",
    "  ) -> jax.Array:\n",
    "    # Tracking of linear velocity commands (xy axes)\n",
    "    local_vel = math.rotate(xd.vel[0], math.quat_inv(x.rot[0]))\n",
    "    lin_vel_error = jp.sum(jp.square(commands[:2] - local_vel[:2]))\n",
    "    lin_vel_reward = jp.exp(\n",
    "        -lin_vel_error / self.reward_config.rewards.tracking_sigma\n",
    "    )\n",
    "    return lin_vel_reward\n",
    "\n",
    "  def _reward_tracking_ang_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion\n",
    "  ) -> jax.Array:\n",
    "    # Tracking of angular velocity commands (yaw)\n",
    "    base_ang_vel = math.rotate(xd.ang[0], math.quat_inv(x.rot[0]))\n",
    "    ang_vel_error = jp.square(commands[2] - base_ang_vel[2])\n",
    "    return jp.exp(-ang_vel_error / self.reward_config.rewards.tracking_sigma)\n",
    "\n",
    "  def _reward_feet_air_time(\n",
    "      self, air_time: jax.Array, first_contact: jax.Array, commands: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # Reward air time.\n",
    "    rew_air_time = jp.sum((air_time - 0.1) * first_contact)\n",
    "    rew_air_time *= (\n",
    "        math.normalize(commands[:2])[1] > 0.05\n",
    "    )  # no reward for zero command\n",
    "    return rew_air_time\n",
    "\n",
    "  def _reward_stand_still(\n",
    "      self,\n",
    "      commands: jax.Array,\n",
    "      joint_angles: jax.Array,\n",
    "  ) -> jax.Array:\n",
    "    # Penalize motion at zero commands\n",
    "    return jp.sum(jp.abs(joint_angles - self._default_pose)) * (\n",
    "        math.normalize(commands[:2])[1] < 0.1\n",
    "    )\n",
    "\n",
    "  def _reward_foot_slip(\n",
    "      self, pipeline_state: base.State, contact_filt: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # get velocities at feet which are offset from lower legs\n",
    "    # pytype: disable=attribute-error\n",
    "    pos = pipeline_state.site_xpos[self._feet_site_id]  # feet position\n",
    "    feet_offset = pos - pipeline_state.xpos[self._lower_leg_body_id]\n",
    "    # pytype: enable=attribute-error\n",
    "    offset = base.Transform.create(pos=feet_offset)\n",
    "    foot_indices = self._lower_leg_body_id - 1  # we got rid of the world body\n",
    "    foot_vel = offset.vmap().do(pipeline_state.xd.take(foot_indices)).vel\n",
    "\n",
    "    # Penalize large feet velocity for feet that are in contact with the ground.\n",
    "    return jp.sum(jp.square(foot_vel[:, :2]) * contact_filt.reshape((-1, 1)))\n",
    "\n",
    "  def _reward_termination(self, done: jax.Array, step: jax.Array) -> jax.Array:\n",
    "    return done & (step < 500)\n",
    "\n",
    "  def render(\n",
    "      self, trajectory: List[base.State], camera: str | None = None,\n",
    "      width: int = 240, height: int = 320,\n",
    "  ) -> Sequence[np.ndarray]:\n",
    "    camera = camera or 'track'\n",
    "    return super().render(trajectory, camera=camera, width=width, height=height)\n",
    "\n",
    "envs.register_environment('barkour', BarkourEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d563e",
   "metadata": {},
   "source": [
    "# Adapt this from PureJaxRL for PPO and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "from wrappers import (\n",
    "    LogWrapper,\n",
    "    BraxGymnaxWrapper,\n",
    "    VecEnv,\n",
    "    NormalizeVecObservation,\n",
    "    NormalizeVecReward,\n",
    "    ClipAction,\n",
    ")\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_logtstd = self.param(\"log_std\", nn.initializers.zeros, (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(actor_mean, jnp.exp(actor_logtstd))\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "\n",
    "def make_train(config):\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    env, env_params = BraxGymnaxWrapper(config[\"ENV_NAME\"]), None\n",
    "    env = LogWrapper(env)\n",
    "    env = ClipAction(env)\n",
    "    env = VecEnv(env)\n",
    "    if config[\"NORMALIZE_ENV\"]:\n",
    "        env = NormalizeVecObservation(env)\n",
    "        env = NormalizeVecReward(env, config[\"GAMMA\"])\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "        # INIT NETWORK\n",
    "        network = ActorCritic(\n",
    "            env.action_space(env_params).shape[0], activation=config[\"ACTIVATION\"]\n",
    "        )\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi, value = network.apply(train_state.params, last_obs)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = env.step(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(\n",
    "                    done, action, value, reward, log_prob, last_obs, info\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_state, env_state, last_obs, rng = runner_state\n",
    "            _, last_val = network.apply(train_state.params, last_obs)\n",
    "\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of envs\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "            if config.get(\"DEBUG\"):\n",
    "\n",
    "                def callback(info):\n",
    "                    return_values = info[\"returned_episode_returns\"][\n",
    "                        info[\"returned_episode\"]\n",
    "                    ]\n",
    "                    timesteps = (\n",
    "                        info[\"timestep\"][info[\"returned_episode\"]] * config[\"NUM_ENVS\"]\n",
    "                    )\n",
    "                    for t in range(len(timesteps)):\n",
    "                        print(\n",
    "                            f\"global step={timesteps[t]}, episodic return={return_values[t]}\"\n",
    "                        )\n",
    "\n",
    "                jax.debug.callback(callback, metric)\n",
    "\n",
    "            runner_state = (train_state, env_state, last_obs, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"LR\": 3e-4,\n",
    "        \"NUM_ENVS\": 2048,\n",
    "        \"NUM_STEPS\": 10,\n",
    "        \"TOTAL_TIMESTEPS\": 5e7,\n",
    "        \"UPDATE_EPOCHS\": 4,\n",
    "        \"NUM_MINIBATCHES\": 32,\n",
    "        \"GAMMA\": 0.99,\n",
    "        \"GAE_LAMBDA\": 0.95,\n",
    "        \"CLIP_EPS\": 0.2,\n",
    "        \"ENT_COEF\": 0.0,\n",
    "        \"VF_COEF\": 0.5,\n",
    "        \"MAX_GRAD_NORM\": 0.5,\n",
    "        \"ACTIVATION\": \"tanh\",\n",
    "        \"ENV_NAME\": \"hopper\",\n",
    "        \"ANNEAL_LR\": False,\n",
    "        \"NORMALIZE_ENV\": True,\n",
    "        \"DEBUG\": True,\n",
    "    }\n",
    "    rng = jax.random.PRNGKey(30)\n",
    "    train_jit = jax.jit(make_train(config))\n",
    "    out = train_jit(rng)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84400b2",
   "metadata": {},
   "source": [
    "# Custom Env + PureJaxRL PPO Integration\n",
    "\n",
    "The key is creating a wrapper that adapts your Brax `PipelineEnv` to the Gymnax interface that PureJaxRL expects.\n",
    "\n",
    "**Interface comparison:**\n",
    "- Brax: `state = env.reset(rng)` / `state = env.step(state, action)` → returns `State` dataclass\n",
    "- Gymnax: `obs, state = env.reset(key, params)` / `obs, state, reward, done, info = env.step(key, state, action, params)` → returns tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8061c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Wrapper to adapt Brax PipelineEnv to Gymnax interface (for PureJaxRL)\n",
    "# =============================================================================\n",
    "\n",
    "from gymnax.environments import spaces as gymnax_spaces\n",
    "from brax.envs.wrappers.training import EpisodeWrapper, AutoResetWrapper\n",
    "\n",
    "\n",
    "class BraxToGymnaxWrapper:\n",
    "    \"\"\"Wraps a Brax PipelineEnv to match Gymnax interface for PureJaxRL.\"\"\"\n",
    "    \n",
    "    def __init__(self, env, episode_length: int = 1000, action_repeat: int = 1):\n",
    "        # Wrap with Brax's episode management and auto-reset\n",
    "        env = EpisodeWrapper(env, episode_length=episode_length, action_repeat=action_repeat)\n",
    "        env = AutoResetWrapper(env)\n",
    "        self._env = env\n",
    "        self.action_size = env.action_size\n",
    "        self.observation_size = env.observation_size\n",
    "    \n",
    "    def reset(self, key, params=None):\n",
    "        \"\"\"Reset environment. Returns (obs, state) tuple.\"\"\"\n",
    "        state = self._env.reset(key)\n",
    "        return state.obs, state\n",
    "    \n",
    "    def step(self, key, state, action, params=None):\n",
    "        \"\"\"Step environment. Returns (obs, state, reward, done, info) tuple.\"\"\"\n",
    "        # Note: key is unused since Brax env stores rng in state.info\n",
    "        next_state = self._env.step(state, action)\n",
    "        done = next_state.done > 0.5  # Convert to boolean\n",
    "        info = next_state.info if hasattr(next_state, 'info') else {}\n",
    "        return next_state.obs, next_state, next_state.reward, done, info\n",
    "    \n",
    "    def observation_space(self, params=None):\n",
    "        return gymnax_spaces.Box(\n",
    "            low=-jnp.inf,\n",
    "            high=jnp.inf,\n",
    "            shape=(self.observation_size,),\n",
    "        )\n",
    "    \n",
    "    def action_space(self, params=None):\n",
    "        return gymnax_spaces.Box(\n",
    "            low=-1.0,\n",
    "            high=1.0,\n",
    "            shape=(self.action_size,),\n",
    "        )\n",
    "\n",
    "\n",
    "# Test the wrapper with your custom env\n",
    "print(\"Testing BraxToGymnaxWrapper with BarkourEnv...\")\n",
    "test_env = BarkourEnv()\n",
    "wrapped_env = BraxToGymnaxWrapper(test_env, episode_length=500)\n",
    "\n",
    "# Quick sanity check\n",
    "rng = jax.random.PRNGKey(0)\n",
    "obs, state = wrapped_env.reset(rng)\n",
    "print(f\"Observation shape: {obs.shape}\")\n",
    "print(f\"Action space: {wrapped_env.action_space(None).shape}\")\n",
    "print(f\"Observation space: {wrapped_env.observation_space(None).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f592998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Additional Wrappers (from PureJaxRL) - simplified inline versions\n",
    "# =============================================================================\n",
    "\n",
    "from flax import struct\n",
    "\n",
    "@struct.dataclass\n",
    "class LogEnvState:\n",
    "    env_state: Any\n",
    "    episode_returns: float\n",
    "    episode_lengths: int\n",
    "    returned_episode_returns: float\n",
    "    returned_episode_lengths: int\n",
    "    timestep: int\n",
    "\n",
    "\n",
    "class LogWrapper:\n",
    "    \"\"\"Log the episode returns and lengths.\"\"\"\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "    def reset(self, key, params=None):\n",
    "        obs, env_state = self._env.reset(key, params)\n",
    "        state = LogEnvState(env_state, 0.0, 0, 0.0, 0, 0)\n",
    "        return obs, state\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state.env_state, action, params\n",
    "        )\n",
    "        new_episode_return = state.episode_returns + reward\n",
    "        new_episode_length = state.episode_lengths + 1\n",
    "        state = LogEnvState(\n",
    "            env_state=env_state,\n",
    "            episode_returns=new_episode_return * (1 - done),\n",
    "            episode_lengths=new_episode_length * (1 - done),\n",
    "            returned_episode_returns=state.returned_episode_returns * (1 - done) + new_episode_return * done,\n",
    "            returned_episode_lengths=state.returned_episode_lengths * (1 - done) + new_episode_length * done,\n",
    "            timestep=state.timestep + 1,\n",
    "        )\n",
    "        info = {\n",
    "            \"returned_episode_returns\": state.returned_episode_returns,\n",
    "            \"returned_episode_lengths\": state.returned_episode_lengths,\n",
    "            \"timestep\": state.timestep,\n",
    "            \"returned_episode\": done,\n",
    "        }\n",
    "        return obs, state, reward, done, info\n",
    "\n",
    "\n",
    "class ClipAction:\n",
    "    \"\"\"Clip actions to [-1, 1] range.\"\"\"\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "    def step(self, key, state, action, params=None):\n",
    "        action = jnp.clip(action, -1.0, 1.0)\n",
    "        return self._env.step(key, state, action, params)\n",
    "\n",
    "\n",
    "class VecEnv:\n",
    "    \"\"\"Vectorize environment with vmap.\"\"\"\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "    def reset(self, keys, params=None):\n",
    "        return jax.vmap(self._env.reset, in_axes=(0, None))(keys, params)\n",
    "\n",
    "    def step(self, keys, states, actions, params=None):\n",
    "        return jax.vmap(self._env.step, in_axes=(0, 0, 0, None))(keys, states, actions, params)\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizeVecObsState:\n",
    "    mean: jnp.ndarray\n",
    "    var: jnp.ndarray\n",
    "    count: float\n",
    "    env_state: Any\n",
    "\n",
    "\n",
    "class NormalizeVecObservation:\n",
    "    \"\"\"Running normalization of observations.\"\"\"\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "    def reset(self, keys, params=None):\n",
    "        obs, env_state = self._env.reset(keys, params)\n",
    "        state = NormalizeVecObsState(\n",
    "            mean=jnp.zeros(obs.shape[-1]),\n",
    "            var=jnp.ones(obs.shape[-1]),\n",
    "            count=1e-4,\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        # Don't normalize on first reset\n",
    "        return obs, state\n",
    "\n",
    "    def step(self, keys, state, actions, params=None):\n",
    "        obs, env_state, reward, done, info = self._env.step(keys, state.env_state, actions, params)\n",
    "        \n",
    "        # Update running stats\n",
    "        batch_mean = jnp.mean(obs, axis=0)\n",
    "        batch_var = jnp.var(obs, axis=0)\n",
    "        batch_count = obs.shape[0]\n",
    "        \n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "        \n",
    "        state = NormalizeVecObsState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        \n",
    "        # Normalize observations\n",
    "        obs = (obs - state.mean) / jnp.sqrt(state.var + 1e-8)\n",
    "        return obs, state, reward, done, info\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class NormalizeVecRewState:\n",
    "    mean: float\n",
    "    var: float\n",
    "    count: float\n",
    "    return_val: jnp.ndarray\n",
    "    env_state: Any\n",
    "\n",
    "\n",
    "class NormalizeVecReward:\n",
    "    \"\"\"Running normalization of rewards.\"\"\"\n",
    "    def __init__(self, env, gamma: float = 0.99):\n",
    "        self._env = env\n",
    "        self._gamma = gamma\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._env, name)\n",
    "\n",
    "    def reset(self, keys, params=None):\n",
    "        obs, env_state = self._env.reset(keys, params)\n",
    "        num_envs = keys.shape[0]\n",
    "        state = NormalizeVecRewState(\n",
    "            mean=0.0,\n",
    "            var=1.0,\n",
    "            count=1e-4,\n",
    "            return_val=jnp.zeros(num_envs),\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        return obs, state\n",
    "\n",
    "    def step(self, keys, state, actions, params=None):\n",
    "        obs, env_state, reward, done, info = self._env.step(keys, state.env_state, actions, params)\n",
    "        \n",
    "        # Update return estimate\n",
    "        return_val = state.return_val * self._gamma * (1 - done) + reward\n",
    "        \n",
    "        # Update running stats\n",
    "        batch_mean = jnp.mean(return_val)\n",
    "        batch_var = jnp.var(return_val)\n",
    "        batch_count = return_val.shape[0]\n",
    "        \n",
    "        delta = batch_mean - state.mean\n",
    "        tot_count = state.count + batch_count\n",
    "        new_mean = state.mean + delta * batch_count / tot_count\n",
    "        m_a = state.var * state.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + jnp.square(delta) * state.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "        new_count = tot_count\n",
    "        \n",
    "        state = NormalizeVecRewState(\n",
    "            mean=new_mean,\n",
    "            var=new_var,\n",
    "            count=new_count,\n",
    "            return_val=return_val,\n",
    "            env_state=env_state,\n",
    "        )\n",
    "        \n",
    "        # Normalize reward\n",
    "        reward = reward / jnp.sqrt(state.var + 1e-8)\n",
    "        return obs, state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Complete PPO Training with Custom Env (PureJaxRL style)\n",
    "# =============================================================================\n",
    "\n",
    "class ActorCriticContinuous(nn.Module):\n",
    "    \"\"\"Actor-Critic network for continuous action spaces.\"\"\"\n",
    "    action_dim: int\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        activation = nn.relu if self.activation == \"relu\" else nn.tanh\n",
    "        \n",
    "        # Actor network\n",
    "        actor_mean = nn.Dense(256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(actor_mean)\n",
    "        \n",
    "        # Learnable log std\n",
    "        actor_logstd = self.param(\"log_std\", nn.initializers.zeros, (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(actor_mean, jnp.exp(actor_logstd))\n",
    "        \n",
    "        # Critic network\n",
    "        critic = nn.Dense(256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(critic)\n",
    "        \n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: Any\n",
    "\n",
    "\n",
    "def make_train_custom_env(config, env_class, env_kwargs=None):\n",
    "    \"\"\"\n",
    "    Create training function for custom PipelineEnv.\n",
    "    \n",
    "    Args:\n",
    "        config: Training hyperparameters dict\n",
    "        env_class: Your custom PipelineEnv class (e.g., BarkourEnv)\n",
    "        env_kwargs: Optional kwargs to pass to env constructor\n",
    "    \"\"\"\n",
    "    env_kwargs = env_kwargs or {}\n",
    "    \n",
    "    config[\"NUM_UPDATES\"] = int(\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    \n",
    "    # Create env with Gymnax wrapper stack\n",
    "    base_env = env_class(**env_kwargs)\n",
    "    env = BraxToGymnaxWrapper(base_env, episode_length=config.get(\"EPISODE_LENGTH\", 1000))\n",
    "    env = LogWrapper(env)\n",
    "    env = ClipAction(env)\n",
    "    env = VecEnv(env)\n",
    "    if config.get(\"NORMALIZE_ENV\", True):\n",
    "        env = NormalizeVecObservation(env)\n",
    "        env = NormalizeVecReward(env, config[\"GAMMA\"])\n",
    "    \n",
    "    env_params = None  # Gymnax params (not used for Brax envs)\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "        # INIT NETWORK\n",
    "        network = ActorCriticContinuous(\n",
    "            action_dim=env.action_space(env_params).shape[0],\n",
    "            activation=config.get(\"ACTIVATION\", \"tanh\"),\n",
    "        )\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        \n",
    "        if config.get(\"ANNEAL_LR\", False):\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "        \n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                pi, value = network.apply(train_state.params, last_obs)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = env.step(\n",
    "                    rng_step, env_state, action, env_params\n",
    "                )\n",
    "                transition = Transition(done, action, value, reward, log_prob, last_obs, info)\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE (GAE)\n",
    "            train_state, env_state, last_obs, rng = runner_state\n",
    "            _, last_val = network.apply(train_state.params, last_obs)\n",
    "\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = transition.done, transition.value, transition.reward\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = delta + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # Value loss\n",
    "                        value_pred_clipped = traj_batch.value + (value - traj_batch.value).clip(\n",
    "                            -config[\"CLIP_EPS\"], config[\"CLIP_EPS\"]\n",
    "                        )\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "\n",
    "                        # Policy loss\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = jnp.clip(ratio, 1.0 - config[\"CLIP_EPS\"], 1.0 + config[\"CLIP_EPS\"]) * gae\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2).mean()\n",
    "                        \n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = loss_actor + config[\"VF_COEF\"] * value_loss - config[\"ENT_COEF\"] * entropy\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(train_state.params, traj_batch, advantages, targets)\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                \n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(lambda x: x.reshape((batch_size,) + x.shape[2:]), batch)\n",
    "                shuffled_batch = jax.tree_util.tree_map(lambda x: jnp.take(x, permutation, axis=0), batch)\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                \n",
    "                train_state, total_loss = jax.lax.scan(_update_minbatch, train_state, minibatches)\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "\n",
    "            # Debug callback\n",
    "            if config.get(\"DEBUG\"):\n",
    "                def callback(info):\n",
    "                    return_values = info[\"returned_episode_returns\"][info[\"returned_episode\"]]\n",
    "                    timesteps = info[\"timestep\"][info[\"returned_episode\"]] * config[\"NUM_ENVS\"]\n",
    "                    for t in range(len(timesteps)):\n",
    "                        print(f\"global step={timesteps[t]}, episodic return={return_values[t]:.2f}\")\n",
    "                jax.debug.callback(callback, metric)\n",
    "\n",
    "            runner_state = (train_state, env_state, last_obs, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = jax.lax.scan(_update_step, runner_state, None, config[\"NUM_UPDATES\"])\n",
    "        \n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f5d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run Training with Custom Env\n",
    "# =============================================================================\n",
    "\n",
    "# Training config\n",
    "config = {\n",
    "    \"LR\": 3e-4,\n",
    "    \"NUM_ENVS\": 2048,              # Number of parallel environments\n",
    "    \"NUM_STEPS\": 10,               # Rollout length per update\n",
    "    \"TOTAL_TIMESTEPS\": 5e7,        # Total training timesteps\n",
    "    \"UPDATE_EPOCHS\": 4,            # PPO epochs per update\n",
    "    \"NUM_MINIBATCHES\": 32,         # Minibatches per epoch\n",
    "    \"GAMMA\": 0.99,                 # Discount factor\n",
    "    \"GAE_LAMBDA\": 0.95,            # GAE lambda\n",
    "    \"CLIP_EPS\": 0.2,               # PPO clip epsilon\n",
    "    \"ENT_COEF\": 0.0,               # Entropy coefficient\n",
    "    \"VF_COEF\": 0.5,                # Value function coefficient\n",
    "    \"MAX_GRAD_NORM\": 0.5,          # Gradient clipping\n",
    "    \"ACTIVATION\": \"tanh\",          # Network activation\n",
    "    \"ANNEAL_LR\": False,            # LR annealing\n",
    "    \"NORMALIZE_ENV\": True,         # Normalize obs/rewards\n",
    "    \"EPISODE_LENGTH\": 500,         # Max episode length\n",
    "    \"DEBUG\": True,                 # Print training progress\n",
    "}\n",
    "\n",
    "# Create training function with your custom env\n",
    "train_fn = make_train_custom_env(\n",
    "    config=config,\n",
    "    env_class=BarkourEnv,\n",
    "    env_kwargs={\n",
    "        \"obs_noise\": 0.05,\n",
    "        \"action_scale\": 0.3,\n",
    "        \"kick_vel\": 0.05,\n",
    "    }\n",
    ")\n",
    "\n",
    "# JIT compile and run\n",
    "rng = jax.random.PRNGKey(42)\n",
    "train_jit = jax.jit(train_fn)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total timesteps: {config['TOTAL_TIMESTEPS']:.0e}\")\n",
    "print(f\"Num envs: {config['NUM_ENVS']}\")\n",
    "print(f\"Num updates: {config['NUM_UPDATES']}\")\n",
    "\n",
    "# Run training\n",
    "out = train_jit(rng)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b72c0",
   "metadata": {},
   "source": [
    "# Template: Creating Your Own Custom Env\n",
    "\n",
    "Below is a simplified template you can use as a starting point for your own custom MJX environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955028ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Template: Custom MJX Environment\n",
    "# =============================================================================\n",
    "\n",
    "class MyCustomEnv(PipelineEnv):\n",
    "    \"\"\"\n",
    "    Template for a custom MJX environment.\n",
    "    Inherit from PipelineEnv and implement reset() and step().\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xml_path: str = \"path/to/your/model.xml\",\n",
    "        action_scale: float = 1.0,\n",
    "        obs_noise: float = 0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Load MuJoCo model\n",
    "        sys = mjcf.load(xml_path)\n",
    "        \n",
    "        # Optional: modify physics parameters\n",
    "        # sys = sys.tree_replace({'opt.timestep': 0.004})\n",
    "        \n",
    "        # Initialize parent class\n",
    "        n_frames = kwargs.pop('n_frames', 5)  # physics steps per control step\n",
    "        super().__init__(sys, backend='mjx', n_frames=n_frames)\n",
    "        \n",
    "        # Store env parameters\n",
    "        self._action_scale = action_scale\n",
    "        self._obs_noise = obs_noise\n",
    "        \n",
    "        # Get initial state from XML keyframe (if available)\n",
    "        # self._init_q = jp.array(sys.mj_model.keyframe('home').qpos)\n",
    "        self._init_q = jp.zeros(sys.nq)  # or set your default\n",
    "        self._nv = sys.nv\n",
    "        \n",
    "        # Define action/observation sizes for the wrapper\n",
    "        self.action_size = sys.nu\n",
    "        self.observation_size = self._get_obs_size()\n",
    "\n",
    "    def _get_obs_size(self) -> int:\n",
    "        \"\"\"Return the size of your observation vector.\"\"\"\n",
    "        # Example: joint positions (nq) + joint velocities (nv) + some extras\n",
    "        return self.sys.nq + self.sys.nv\n",
    "\n",
    "    def reset(self, rng: jax.Array) -> State:\n",
    "        \"\"\"Reset environment to initial state.\"\"\"\n",
    "        rng, key = jax.random.split(rng)\n",
    "        \n",
    "        # Initialize physics state\n",
    "        qpos = self._init_q  # + optional noise\n",
    "        qvel = jp.zeros(self._nv)\n",
    "        pipeline_state = self.pipeline_init(qpos, qvel)\n",
    "        \n",
    "        # Build observation\n",
    "        obs = self._get_obs(pipeline_state, rng)\n",
    "        \n",
    "        # State info dict (store anything you need across steps)\n",
    "        state_info = {\n",
    "            'rng': rng,\n",
    "            'step': 0,\n",
    "            'last_action': jp.zeros(self.action_size),\n",
    "        }\n",
    "        \n",
    "        reward, done = jp.zeros(2)\n",
    "        metrics = {}  # Optional logged metrics\n",
    "        \n",
    "        return State(pipeline_state, obs, reward, done, metrics, state_info)\n",
    "\n",
    "    def step(self, state: State, action: jax.Array) -> State:\n",
    "        \"\"\"Take one environment step.\"\"\"\n",
    "        rng, _ = jax.random.split(state.info['rng'])\n",
    "        \n",
    "        # Scale and apply action\n",
    "        action = action * self._action_scale\n",
    "        pipeline_state = self.pipeline_step(state.pipeline_state, action)\n",
    "        \n",
    "        # Compute observation\n",
    "        obs = self._get_obs(pipeline_state, rng)\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = self._compute_reward(pipeline_state, action, state)\n",
    "        \n",
    "        # Check termination\n",
    "        done = self._is_done(pipeline_state)\n",
    "        \n",
    "        # Update state info\n",
    "        state_info = {\n",
    "            'rng': rng,\n",
    "            'step': state.info['step'] + 1,\n",
    "            'last_action': action,\n",
    "        }\n",
    "        \n",
    "        return state.replace(\n",
    "            pipeline_state=pipeline_state,\n",
    "            obs=obs,\n",
    "            reward=reward,\n",
    "            done=jp.float32(done),\n",
    "            info=state_info,\n",
    "        )\n",
    "\n",
    "    def _get_obs(self, pipeline_state, rng) -> jax.Array:\n",
    "        \"\"\"Build observation vector from physics state.\"\"\"\n",
    "        obs = jp.concatenate([\n",
    "            pipeline_state.q,   # joint positions\n",
    "            pipeline_state.qd,  # joint velocities\n",
    "        ])\n",
    "        \n",
    "        # Optional: add observation noise\n",
    "        if self._obs_noise > 0:\n",
    "            obs = obs + self._obs_noise * jax.random.uniform(\n",
    "                rng, obs.shape, minval=-1, maxval=1\n",
    "            )\n",
    "        return obs\n",
    "\n",
    "    def _compute_reward(self, pipeline_state, action, state) -> jax.Array:\n",
    "        \"\"\"Compute reward for current state.\"\"\"\n",
    "        # Example: reward forward velocity, penalize energy\n",
    "        # Customize this for your task!\n",
    "        \n",
    "        # forward_vel = pipeline_state.qd[0]  # example\n",
    "        # energy_penalty = jp.sum(jp.square(action)) * 0.01\n",
    "        # reward = forward_vel - energy_penalty\n",
    "        \n",
    "        reward = jp.array(0.0)  # placeholder\n",
    "        return reward\n",
    "\n",
    "    def _is_done(self, pipeline_state) -> jax.Array:\n",
    "        \"\"\"Check if episode should terminate.\"\"\"\n",
    "        # Example: terminate if robot falls\n",
    "        # height = pipeline_state.x.pos[0, 2]\n",
    "        # done = height < 0.3\n",
    "        \n",
    "        done = jp.array(False)  # placeholder\n",
    "        return done\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# my_env = MyCustomEnv(xml_path=\"my_robot.xml\", action_scale=0.5)\n",
    "# train_fn = make_train_custom_env(config, MyCustomEnv, {\"xml_path\": \"my_robot.xml\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5dd626",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define your env (inheriting from PipelineEnv)\n",
    "class MyEnv(PipelineEnv):\n",
    "    ...\n",
    "\n",
    "# 2. Create training function\n",
    "train_fn = make_train_custom_env(config, MyEnv, env_kwargs)\n",
    "\n",
    "# 3. JIT and run\n",
    "out = jax.jit(train_fn)(rng)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
