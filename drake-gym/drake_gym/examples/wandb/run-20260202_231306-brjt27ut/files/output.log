Using cuda device
/home/xiao/pyenvs/robotics/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
Logging to runs/brjt27ut/PPO_1
[2K---------------------------------â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m12,270/3,000,000 [0m [ [33m0:00:20[0m < [36m1:21:59[0m , [31m607 it/s[0m ]
| rollout/           |          |
|    ep_len_mean     | 3.9      |
|    ep_rew_mean     | 8.17     |
| time/              |          |
|    fps             | 601      |
|    iterations      | 1        |
|    time_elapsed    | 20       |
|    total_timesteps | 12288    |
---------------------------------
[2K-----------------------------------------8;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m24,522/3,000,000 [0m [ [33m0:00:45[0m < [36m1:36:21[0m , [31m515 it/s[0m ]
| rollout/                |             |
|    ep_len_mean          | 4.2         |
|    ep_rew_mean          | 7.29        |
| time/                   |             |
|    fps                  | 535         |
|    iterations           | 2           |
|    time_elapsed         | 45          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.008386027 |
|    clip_fraction        | 0.0855      |
|    clip_range           | 0.2         |
|    entropy_loss         | -9.91       |
|    explained_variance   | 0.00407     |
|    learning_rate        | 0.0003      |
|    loss                 | 718         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00272    |
|    std                  | 0.998       |
|    value_loss           | 1.34e+03    |
-----------------------------------------
[2KTraceback (most recent call last):[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m27,828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
[2K  File "/home/xiao/0_codes/my_rl/drake-gym/drake_gym/examples/train_franka_reach_PPO.py", line 269, in <module>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m27,828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
    sys.exit(main())
[2K  File "/home/xiao/0_codes/my_rl/drake-gym/drake_gym/examples/train_franka_reach_PPO.py", line 261, in mainâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m27,828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
    model.learn(
[2K  File "/home/xiao/pyenvs/robotics/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learnâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m27,828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
    return super().learn(
[2K  File "/home/xiao/pyenvs/robotics/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 323, in learn [32m27,828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
[2K  File "/home/xiao/pyenvs/robotics/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 218, in â”[0m [32m27,828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
[2K  File "/home/xiao/pyenvs/robotics/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 206, in step [32m27,828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
    return self.step_wait()
[2K  File "/home/xiao/pyenvs/robotics/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py", line 128, in step_wait,828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
    results = [remote.recv() for remote in self.remotes]
[2K  File "/home/xiao/pyenvs/robotics/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py", line 128, in <listcomp>828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
    results = [remote.recv() for remote in self.remotes]
[2K  File "/usr/lib/python3.10/multiprocessing/connection.py", line 250, in recvâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m27,828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
    buf = self._recv_bytes()
[2K  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytesâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m27,828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
    buf = self._recv(4)
[2K  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recvâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m27,828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
    chunk = read(handle, remaining)
[2KKeyboardInterrupt;249;38;114mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m27,828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
[35m   1%[0m [38;2;249;38;114mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m27,828/3,000,000 [0m [ [33m0:00:58[0m < [36m1:47:31[0m , [31m461 it/s[0m ]
